{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#starting with logistic model code\n",
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = tf_train_labels)) + beta_regul * tf.nn.l2_loss(weights)\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Minibatch loss at step 0: 23.441673\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 13.8%\n",
      "Minibatch loss at step 500: 2.470273\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 75.5%\n",
      "Minibatch loss at step 1000: 1.694261\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 78.3%\n",
      "Minibatch loss at step 1500: 1.015143\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 2000: 0.847980\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 2500: 0.827478\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 3000: 0.799368\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.8%\n",
      "Test accuracy: 88.8%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2 introduces a new parameter, that needs to be tuned. Plotting the accuracy(log scale) might tell us something.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x182e0f13c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (logistic)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying the above technique to the one-hidden layer network we have...\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = tf_train_labels)) + \\\n",
    "      beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 688.575195\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 33.6%\n",
      "Minibatch loss at step 500: 203.472763\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 79.7%\n",
      "Minibatch loss at step 1000: 115.766594\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 1500: 68.341118\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 2000: 41.200363\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step 2500: 25.142813\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 3000: 15.490564\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.4%\n",
      "Test accuracy: 93.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the final accuracy by the L2 parameter to find the best value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:    \n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for step in range(num_steps):\n",
    "      # Pick an offset within the training data, which has been randomized.\n",
    "      # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "      # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "      # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "      # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "      # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18413cf710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (1-layer net)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 345.248566\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 29.5%\n",
      "Minibatch loss at step 100: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 200: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Test accuracy: 82.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 201\n",
    "num_batches = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    offset = ((step % num_batches) * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 100 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  drop1 = tf.nn.dropout(lay1_train, 0.3)\n",
    "  logits = tf.matmul(drop1, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 678.084473\n",
      "Minibatch accuracy: 14.8%\n",
      "Validation accuracy: 33.7%\n",
      "Minibatch loss at step 100: 23.748045\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 70.3%\n",
      "Minibatch loss at step 200: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.3%\n",
      "Test accuracy: 77.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 201\n",
    "num_batches = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    offset = step % num_batches\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 100 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In theory, the accuracy is suppose to improve. but my tests conclude that the accuracy has dropped. I will have to look into them further as see whats happening where.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 512\n",
    "beta_regul = 1e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "  logits = tf.matmul(lay2_train, weights3) + biases3\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = tf_train_labels)) + \\\n",
    "      beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3))\n",
    "  \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.65, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay2_test, weights3) + biases3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.525213\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 33.4%\n",
      "Minibatch loss at step 2500: 0.583972\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 5000: 0.541183\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 7500: 0.499313\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.2%\n",
      "Test accuracy: 95.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 9001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 2500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, Lets add another layer, try dropout once or twice and try optimizing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 512\n",
    "num_hidden_nodes3 = 256\n",
    "beta_regul = 1e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_hidden_nodes3], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_hidden_nodes3]))\n",
    "  weights4 = tf.Variable(\n",
    "      tf.truncated_normal([num_hidden_nodes3, num_labels], stddev = np.sqrt(2.0 / num_hidden_nodes3)))\n",
    "  biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "  drop2 = tf.nn.dropout(lay2_train, 0.5)\n",
    "  lay3_train = tf.nn.relu(tf.matmul(drop2, weights3) + biases3)\n",
    "  logits = tf.matmul(lay3_train, weights4) + biases4\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = tf_train_labels))\n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 4000, 0.65, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay3_valid = tf.nn.relu(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay3_valid, weights4) + biases4)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  lay3_test = tf.nn.relu(tf.matmul(lay2_test, weights3) + biases3)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay3_test, weights4) + biases4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.447145\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 30.5%\n",
      "Minibatch loss at step 5000: 0.351898\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 10000: 0.238975\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 15000: 0.146696\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.0%\n",
      "Test accuracy: 96.4%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 18001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 5000 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
